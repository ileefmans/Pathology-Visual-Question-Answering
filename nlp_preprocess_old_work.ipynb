{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nylaennels/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list=['What year was pathology founded?', 'What does pathology mean?', 'Name five viral diseases',\n",
    "             'Where is the liver located?', 'What does the liver do?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "for j in range(len(doc_list)):\n",
    "    doc_list[j] = text_to_word_sequence(doc_list[j])\n",
    "    for i in stop_words:\n",
    "        count=0\n",
    "        while count==0:\n",
    "            try:\n",
    "                doc_list[j].remove(i)\n",
    "            except:\n",
    "                count+=1\n",
    "            finally:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['year', 'pathology', 'founded'],\n",
       " ['pathology', 'mean'],\n",
       " ['name', 'five', 'viral', 'diseases'],\n",
       " ['liver', 'located'],\n",
       " ['liver']]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(oov_token='<UNK>') #oov_token can be removed if determined to not be neccessary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "for i in range(len(doc_list)): #i = question index (i.e. question1, question2, etc...)\n",
    "    for j in range(len(doc_list[i])):\n",
    "        doc_list[i][j] = (ps.stem(doc_list[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['year', 'patholog', 'found'],\n",
       " ['patholog', 'mean'],\n",
       " ['name', 'five', 'viral', 'diseas'],\n",
       " ['liver', 'locat'],\n",
       " ['liver']]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.fit_on_texts(doc_list)\n",
    "matrx = tok.texts_to_matrix(tokens, mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 12)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['year', 'patholog', 'found'],\n",
       " ['patholog', 'mean'],\n",
       " ['name', 'five', 'viral', 'diseas'],\n",
       " ['liver', 'locat'],\n",
       " ['liver']]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 3.76020844, 0.98082925, 0.        , 1.25276297,\n",
       "        1.25276297, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 3.76020844, 0.98082925, 0.        , 0.        ,\n",
       "        0.        , 1.25276297, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.25276297, 1.25276297, 1.25276297,\n",
       "        1.25276297, 0.        ],\n",
       "       [0.        , 4.27566552, 0.        , 0.98082925, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.25276297],\n",
       "       [0.        , 4.67548509, 0.        , 0.98082925, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_words': None,\n",
       " 'filters': '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
       " 'lower': True,\n",
       " 'split': ' ',\n",
       " 'char_level': False,\n",
       " 'oov_token': '<UNK>',\n",
       " 'document_count': 5,\n",
       " 'word_counts': '{\"year\": 1, \"patholog\": 2, \"found\": 1, \"mean\": 1, \"name\": 1, \"five\": 1, \"viral\": 1, \"diseas\": 1, \"liver\": 2, \"locat\": 1}',\n",
       " 'word_docs': '{\"found\": 1, \"year\": 1, \"patholog\": 2, \"mean\": 1, \"diseas\": 1, \"viral\": 1, \"five\": 1, \"name\": 1, \"liver\": 2, \"locat\": 1}',\n",
       " 'index_docs': '{\"5\": 1, \"4\": 1, \"2\": 2, \"6\": 1, \"10\": 1, \"9\": 1, \"8\": 1, \"7\": 1, \"3\": 2, \"11\": 1}',\n",
       " 'index_word': '{\"1\": \"<UNK>\", \"2\": \"patholog\", \"3\": \"liver\", \"4\": \"year\", \"5\": \"found\", \"6\": \"mean\", \"7\": \"name\", \"8\": \"five\", \"9\": \"viral\", \"10\": \"diseas\", \"11\": \"locat\"}',\n",
       " 'word_index': '{\"<UNK>\": 1, \"patholog\": 2, \"liver\": 3, \"year\": 4, \"found\": 5, \"mean\": 6, \"name\": 7, \"five\": 8, \"viral\": 9, \"diseas\": 10, \"locat\": 11}'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year\n",
      "pathology\n",
      "founded\n",
      "pathology\n",
      "mean\n",
      "name\n",
      "five\n",
      "viral\n",
      "diseases\n",
      "liver\n",
      "located\n",
      "liver\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(doc_list)): #i = question index (i.e. question1, question2, etc...)\n",
    "    for j in range(len(doc_list[i])):\n",
    "        print(doc_list[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['year', 'pathology', 'founded'],\n",
       " ['pathology', 'mean'],\n",
       " ['name', 'five', 'viral', 'diseases'],\n",
       " ['liver', 'located'],\n",
       " ['liver']]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[]\n",
    "for i in range(len(doc_list)):\n",
    "    tokens.append(word_tokenize(doc_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens = list(tok.word_counts)\n",
    "        #BELOW: tokens = doc_list\n",
    "        \n",
    "        ## Stem words \n",
    "        \n",
    "ps = PorterStemmer()\n",
    "for i in range(len(tokens)): #i = question index (i.e. question1, question2, etc...)\n",
    "    for j in range(len(tokens[i])):\n",
    "        tokens[i][j] = (ps.stem(tokens[i][j]))\n",
    "        \n",
    "        ## Return tf-idf matrix\n",
    "        \n",
    "matrx = tok.texts_to_matrix(tokens, mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 19)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['what', 'year', 'wa', 'patholog', 'found', '?'],\n",
       " ['what', 'doe', 'patholog', 'mean', '?'],\n",
       " ['name', 'five', 'viral', 'diseas'],\n",
       " ['where', 'is', 'the', 'liver', 'locat', '?'],\n",
       " ['what', 'doe', 'the', 'liver', 'do', '?']]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 4.27566552, 0.81093022, 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.25276297, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 3.76020844, 0.81093022, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.25276297, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 1.79175947, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.25276297, 1.25276297, 1.25276297, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 3.03371249, 0.        , 0.        , 0.        ,\n",
       "        0.98082925, 0.98082925, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.25276297, 1.25276297, 0.        , 0.        ],\n",
       "       [0.        , 3.03371249, 0.81093022, 0.        , 0.        ,\n",
       "        0.98082925, 0.98082925, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.25276297]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_words': None,\n",
       " 'filters': '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
       " 'lower': True,\n",
       " 'split': ' ',\n",
       " 'char_level': False,\n",
       " 'oov_token': '<UNK>',\n",
       " 'document_count': 5,\n",
       " 'word_counts': '{\"what\": 3, \"year\": 1, \"was\": 1, \"pathology\": 2, \"founded\": 1, \"does\": 2, \"mean\": 1, \"name\": 1, \"five\": 1, \"viral\": 1, \"diseases\": 1, \"where\": 1, \"is\": 1, \"the\": 2, \"liver\": 2, \"located\": 1, \"do\": 1}',\n",
       " 'word_docs': '{\"pathology\": 2, \"founded\": 1, \"year\": 1, \"was\": 1, \"what\": 3, \"mean\": 1, \"does\": 2, \"viral\": 1, \"five\": 1, \"name\": 1, \"diseases\": 1, \"liver\": 2, \"located\": 1, \"where\": 1, \"the\": 2, \"is\": 1, \"do\": 1}',\n",
       " 'index_docs': '{\"3\": 2, \"9\": 1, \"7\": 1, \"8\": 1, \"2\": 3, \"10\": 1, \"4\": 2, \"13\": 1, \"12\": 1, \"11\": 1, \"14\": 1, \"6\": 2, \"17\": 1, \"15\": 1, \"5\": 2, \"16\": 1, \"18\": 1}',\n",
       " 'index_word': '{\"1\": \"<UNK>\", \"2\": \"what\", \"3\": \"pathology\", \"4\": \"does\", \"5\": \"the\", \"6\": \"liver\", \"7\": \"year\", \"8\": \"was\", \"9\": \"founded\", \"10\": \"mean\", \"11\": \"name\", \"12\": \"five\", \"13\": \"viral\", \"14\": \"diseases\", \"15\": \"where\", \"16\": \"is\", \"17\": \"located\", \"18\": \"do\"}',\n",
       " 'word_index': '{\"<UNK>\": 1, \"what\": 2, \"pathology\": 3, \"does\": 4, \"the\": 5, \"liver\": 6, \"year\": 7, \"was\": 8, \"founded\": 9, \"mean\": 10, \"name\": 11, \"five\": 12, \"viral\": 13, \"diseases\": 14, \"where\": 15, \"is\": 16, \"located\": 17, \"do\": 18}'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class text_process:\n",
    "    \"\"\"\n",
    "    Class for preprocessing text.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,text):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text : a pd column of 'documents' to be vectorized \n",
    "            \n",
    "            \"\"\"\n",
    "        self.text = text\n",
    "        \n",
    "    \n",
    "    def pd_col_to_list(self):\n",
    "        \"\"\"\n",
    "        \n",
    "            Converts pandas column to a LIST of comma-separated text ('documents') before it is to be vectorized.\n",
    "            For example, index 0 of the list = the first question (or answer), and so on.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        doc_list = []\n",
    "        for i in self.text:\n",
    "            doc_list.append(i)\n",
    "            \n",
    "        return doc_list\n",
    "        \n",
    "        \n",
    "    def remove_stopwords(self, doc_list):    \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            doc_list: output from pd_col_to_list, a list of comma separated text ('documents')\n",
    "        \n",
    "            Removes stop_words, according to the stop words nltk corpus, before the list is to be vectorized.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        stop_words = stopwords.words('english')\n",
    "        for j in range(len(doc_list)):\n",
    "            doc_list[j] = text_to_word_sequence(doc_list[j])\n",
    "            for i in stop_words:\n",
    "                count=0\n",
    "                while count==0:\n",
    "                    try:\n",
    "                        doc_list[j].remove(i)\n",
    "                    except:\n",
    "                        count+=1\n",
    "                    finally:\n",
    "                        pass\n",
    "        \n",
    "        return doc_list\n",
    "        \n",
    "\n",
    "    def text_to_vec(self, doc_list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            doc_list: output from remove_stopwords(), a list of comma separated text ('documents') where stop words\n",
    "                       have been removed\n",
    "        \n",
    "        \n",
    "            Converts list of comma-separated 'documents' to a vector, using tf-idf. This is done after stop words \n",
    "            have been removed (with remove_stopwords()). Utilizes the keras tokenizer, which separates on spaces, \n",
    "            makes all tokens lowercase and removes punctuation. Function then stems the tokens, using the \n",
    "            PorterStemmer from nltk. Finally, it returns the preprocessed tokens in vector form.\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        ## Keras tokenizer\n",
    "        \n",
    "        tok = Tokenizer(oov_token='<UNK>')\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## Stem words \n",
    "        \n",
    "        ps = PorterStemmer()\n",
    "        for i in range(len(doc_list)): #i = question index (i.e. question1, question2, etc...)\n",
    "            doc_list[i] = (ps.stem(doc_list[i]))\n",
    "        \n",
    "        ## Fit on doc list & return tf-idf matrix\n",
    "        tok.fit_on_texts(doc_list)\n",
    "        matrx = tok.texts_to_matrix(doc_list, mode='tfidf')\n",
    "        \n",
    "        return matrx, tok.get_config(), doc_list\n",
    "    \n",
    "    \n",
    "    def text_preprocess(self):\n",
    "\n",
    "        list_form = text_process.pd_col_to_list(self)\n",
    "        no_stops = text_process.remove_stopwords(self,list_form)\n",
    "        final_vec = text_process.text_to_vec(self,no_stops)\n",
    "        \n",
    "        return final_vec\n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
